<!DOCTYPE html> <html><head>
		<title>MICCAI'24 Recap</title>
		<base href="../../">
		<meta id="root-path" root-path="../../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="Notes - MICCAI'24 Recap">
		<meta property="og:title" content="MICCAI'24 Recap">
		<meta property="og:description" content="Notes - MICCAI'24 Recap">
		<meta property="og:type" content="website">
		<meta property="og:url" content="conference-meeting-notes/miccai'24/miccai'24-recap.html">
		<meta property="og:image" content="conference-meeting-notes/miccai'24/images/a-framework-for-assessing-joint-human-ai-systems-based-on-uncertainty-estimation.jpg">
		<meta property="og:site_name" content="Notes">
		<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="lib/scripts/pixi.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="lib/scripts/minisearch.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager show-inline-title show-ribbon theme-dark"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles"></style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="MICCAI'24 Recap">MICCAI'24 Recap</h1><div class="heading-wrapper"><h2 data-heading="A framework for assessing joint human-AI systems based on uncertainty estimation" dir="auto" class="heading" id="A_framework_for_assessing_joint_human-AI_systems_based_on_uncertainty_estimation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>A framework for assessing joint human-AI systems based on uncertainty estimation</h2><div class="heading-children"><div><p dir="auto">We investigate the role of uncertainty quantification in aiding medical decision-making. Existing evaluation metrics fail to capture the practical utility of joint human-AI decision-making systems. To address this, we introduce a novel framework to assess such systems and use it to benchmark a diverse set of confidence and uncertainty estimation methods. Our results show that certainty measures enable joint human-AI systems to outperform both standalone humans and AIs, and that for a given system there exists an optimal balance in the number of cases to refer to humans, beyond which the system’s performance degrades.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1535_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1535_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/016-Paper1535.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/016-Paper1535.html" target="_blank">Full Review</a></p></div><div><p dir="auto"><span alt="A framework for assessing joint human-AI systems based on uncertainty estimation.jpg" src="A framework for assessing joint human-AI systems based on uncertainty estimation.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="A framework for assessing joint human-AI systems based on uncertainty estimation.jpg" src="conference-meeting-notes/miccai'24/images/a-framework-for-assessing-joint-human-ai-systems-based-on-uncertainty-estimation.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels" dir="auto" class="heading" id="Fair_and_Accurate_Skin_Disease_Image_Classification_by_Alignment_with_Clinical_Labels"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels</h2><div class="heading-children"><div><p dir="auto">Deep learning models have achieved great success in automating skin lesion diagnosis. However, the ethnic disparity in these models’ predictions needs to be addressed before deployment of these models. We introduce a novel approach: PatchAlign, to enhance skin condition image classification accuracy and fairness through alignment with clinical text representations of skin conditions. PatchAlign uses Graph Optimal Transport (\texttt{GOT}) Loss as a regularizer to perform cross-domain alignment. The representations thus obtained are robust and generalize well across skin tones, even with limited training samples. To reduce the effect of noise/artifacts in clinical dermatology images, we propose a learnable Masked Graph Optimal Transport for cross-domain alignment that further improves the fairness metrics.</p></div><div><p dir="auto">We compare our model to the SOTA model (FairDisCo) on two skin lesion datasets with different skin types: Fitzpatrick17k and Diverse Dermatology Images (DDI). Our proposed approach, PatchAlign, enhances the accuracy of skin condition image classification by 2.8% (in-domain) and 6.2% (out-domain) on Fitzpatrick17k and 4.2% (in-domain) on DDI compared to FairDisCo. In addition, it consistently improves the fairness of true positive rates across skin tones in all of our experiments.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2609_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2609_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/308-Paper2609.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/308-Paper2609.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/aayushmanace/PatchAlign24" rel="noopener" class="external-link" href="https://github.com/aayushmanace/PatchAlign24" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels.jpg" src="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels.jpg" src="conference-meeting-notes/miccai'24/images/fair-and-accurate-skin-disease-image-classification-by-alignment-with-clinical-labels.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="FairQuantize: Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis" dir="auto" class="heading" id="FairQuantize:_Achieving_Fairness_Through_Weight_Quantization_for_Dermatological_Disease_Diagnosis"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>FairQuantize: Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis</h2><div class="heading-children"><div><p dir="auto">Recent studies have demonstrated that deep learning (DL) models for medical image classification may exhibit biases toward certain demographic attributes such as race, gender, and age. Existing bias mitigation strategies often require sensitive attributes for inference, which may not always be available, or achieve moderate fairness enhancement at the cost of significant accuracy decline. To overcome these obstacles, we propose FairQuantize, a novel approach that ensures fairness by quantizing model weights. We reveal that quantization can be used not as a tool for model compression but as a means to improve model fairness. It is based on the observation that different weights in a model impact performance on various demographic groups differently. FairQuantize selectively quantizes certain weights to enhance fairness while only marginally impacting accuracy. In addition, resulting quantized models can work without sensitive attributes as input. Experimental results on two skin disease datasets demonstrate that FairQuantize can significantly enhance fairness among sensitive attributes while minimizing the impact on overall performance.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3697_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3697_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/310-Paper3697.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/310-Paper3697.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/guoyb17/FairQuantize" rel="noopener" class="external-link" href="https://github.com/guoyb17/FairQuantize" target="_blank">Code</a><br>
<span alt="FairQuantize Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis.jpg" src="FairQuantize Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="FairQuantize Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis.jpg" src="conference-meeting-notes/miccai'24/images/fairquantize-achieving-fairness-through-weight-quantization-for-dermatological-disease-diagnosis.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="SkinCON: Towards consensus for the uncertainty of skin cancer sub-typing through distribution regularized adaptive predictive sets (DRAPS)" dir="auto" class="heading" id="SkinCON:_Towards_consensus_for_the_uncertainty_of_skin_cancer_sub-typing_through_distribution_regularized_adaptive_predictive_sets_(DRAPS)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>SkinCON: Towards consensus for the uncertainty of skin cancer sub-typing through distribution regularized adaptive predictive sets (DRAPS)</h2><div class="heading-children"><div><p dir="auto">Deep learning has been widely utilized in medical diagnosis. Convolutional neural networks and transformers can achieve high predictive accuracy, which can be on par with or even exceed human performance. However, uncertainty quantification remains an unresolved issue, impeding the deployment of deep learning models in practical settings. Conformal analysis can, in principle, estimate the uncertainty of each diagnostic prediction, but doing so effectively requires extensive human annotations to characterize the underlying empirical distributions. This has been challenging in the past because instance-level class distribution data has been unavailable: Collecting massive ground truth labels is already challenging, and obtaining the class distribution of each instance is even more difficult. Here, we provide a large skin cancer instance-level class distribution dataset, SkinCON, that contains 25,331 skin cancer images from the ISIC 2019 challenge dataset. SkinCON is built upon over 937,167 diagnostic judgments from 10,509 participants. Using SkinCON, we propose the distribution regularized adaptive predictive sets (DRAPS) method for skin cancer diagnosis. We also provide a new evaluation metric based on SkinCON. Experiment results show the quality of our proposed DRAPS method and the uncertainty variation with respect to patient age and sex from health equity and fairness perspective. The dataset and code are available at <a data-tooltip-position="top" aria-label="https://skincon.github.io." rel="noopener" class="external-link" href="https://skincon.github.io." target="_blank">https://skincon.github.io</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/4090_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/4090_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/708-Paper4090.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/708-Paper4090.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://skincon.github.io." rel="noopener" class="external-link" href="https://skincon.github.io." target="_blank">Code</a></p></div><div><p dir="auto"><span alt="SkinCon.jpg" src="SkinCon.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="SkinCon.jpg" src="conference-meeting-notes/miccai'24/images/skincon.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="MedSynth: Leveraging Generative Model for Healthcare Data Sharing" dir="auto" class="heading" id="MedSynth:_Leveraging_Generative_Model_for_Healthcare_Data_Sharing"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MedSynth: Leveraging Generative Model for Healthcare Data Sharing</h2><div class="heading-children"><div><p dir="auto">Sharing medical datasets among healthcare organizations is essential for advancing AI-assisted disease diagnostics and enhancing patient care. Employing techniques like data de-identification and data synthesis in medical data sharing, however, comes with inherent drawbacks that may lead to privacy leakage. Therefore, there is a pressing need for mechanisms that can effectively conceal sensitive information, ensuring a secure environment for data sharing. Dataset Condensation (DC) emerges as a solution, creating a reduced-scale synthetic dataset from a larger original dataset while maintaining comparable training outcomes. This approach offers advantages in terms of privacy and communication efficiency in the context of medical data sharing. Despite these benefits, traditional condensation methods encounter challenges, particularly with high-resolution medical datasets. To address these challenges, we present MedSynth, a novel dataset condensation scheme designed to efficiently condense the knowledge within extensive medical datasets into a generative model. This facilitates the sharing of the generative model across hospitals without the need to disclose raw data. By combining an attention-based generator with a vision transformer (ViT), MedSynth creates a generative model capable of producing a concise set of representative synthetic medical images, encapsulating the features of the original dataset. This generative model can then be shared with hospitals to optimize various downstream model training tasks. Extensive experimental results across medical datasets demonstrate that MedSynth outperforms state-of-the-art methods. Moreover, MedSynth successfully defends against state-of-the-art Membership Inference Attacks (MIA), highlighting its significant potential in preserving the privacy of medical data.  </p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2872_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2872_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/506-Paper2872.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/506-Paper2872.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="MedSynth.jpg" src="MedSynth.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="MedSynth.jpg" src="conference-meeting-notes/miccai'24/images/medsynth.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images" dir="auto" class="heading" id="S-SYNTH:_Knowledge-Based,_Synthetic_Generation_of_Skin_Images"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images</h2><div class="heading-children"><div><p dir="auto">Development of artificial intelligence (AI) techniques in medical imaging requires access to large-scale and diverse datasets for training and evaluation. In dermatology, obtaining such datasets remains challenging due to significant variations in patient populations, illumination conditions, and acquisition system characteristics. In this work, we propose S-SYNTH, the first knowledge-based, adaptable open-source skin simulation framework to rapidly generate synthetic skin, 3D models and digitally rendered images, using an anatomically inspired multi-layer, multi-component skin and growing lesion model. The skin model allows for controlled variation in skin appearance, such as skin color, presence of hair, lesion shape, and blood fraction among other parameters. We use this framework to study the effect of possible variations on the development and evaluation of AI models for skin lesion segmentation, and show that results obtained using synthetic data follow similar comparative trends as real dermatologic images, while mitigating biases and limitations from existing datasets including small dataset size, lack of diversity, and underrepresentation.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1426_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1426_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/729-Paper1426.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/729-Paper1426.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/DIDSR/ssynth-release" rel="noopener" class="external-link" href="https://github.com/DIDSR/ssynth-release" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="S-Synth.jpg" src="S-Synth.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="S-Synth.jpg" src="conference-meeting-notes/miccai'24/images/s-synth.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis" dir="auto" class="heading" id="AdaCBM:_An_Adaptive_Concept_Bottleneck_Model_for_Explainable_and_Accurate_Diagnosis"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis</h2><div class="heading-children"><div><p dir="auto">The integration of vision-language models such as CLIP and Concept Bottleneck Models (CBMs) offers a promising approach to explaining deep neural network (DNN) decisions using concepts understandable by humans, addressing the black-box concern of DNNs. While CLIP provides both explainability and zero-shot classification capability, its pre-training on generic image and text data may limit its classification accuracy and applicability to medical image diagnostic tasks, creating a transfer learning problem. To maintain explainability and address transfer learning needs, CBM methods commonly design post-processing modules after the bottleneck module. However, this way has been ineffective. This paper takes an unconventional approach by re-examining the CBM framework through the lens of its geometrical representation as a simple linear classification system. The analysis uncovers that post-CBM fine-tuning modules merely rescale and shift the classification outcome of the system, failing to fully leverage the system’s learning potential. We introduce an adaptive module strategically positioned between CLIP and CBM to bridge the gap between source and downstream domains. This simple yet effective approach enhances classification performance while preserving the explainability afforded by the framework. Our work offers a comprehensive solution that encompasses the entire process, from concept discovery to model training, providing a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3895_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3895_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/045-Paper3895.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/045-Paper3895.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="%5Bhttps://github.com/AIML-MED/AdaCBM%5D(https://github.com/AIML-MED/AdaCBM)" rel="noopener" class="external-link" href="%5Bhttps://github.com/AIML-MED/AdaCBM%5D(https://github.com/AIML-MED/AdaCBM)" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="AdaCBM.jpg" src="AdaCBM.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="AdaCBM.jpg" src="conference-meeting-notes/miccai'24/images/adacbm.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Weakly-supervised Medical Image Segmentation with Gaze Annotations" dir="auto" class="heading" id="Weakly-supervised_Medical_Image_Segmentation_with_Gaze_Annotations"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Weakly-supervised Medical Image Segmentation with Gaze Annotations</h2><div class="heading-children"><div><p dir="auto">Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at: <a rel="noopener" class="external-link" href="https://github.com/med-air/GazeMedSeg" target="_blank">https://github.com/med-air/GazeMedSeg</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1675_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1675_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/843-Paper1675.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/843-Paper1675.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/med-air/GazeMedSeg" rel="noopener" class="external-link" href="https://github.com/med-air/GazeMedSeg" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Weaklysupervised segmentation using gaze annotations.jpg" src="Weaklysupervised segmentation using gaze annotations.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Weaklysupervised segmentation using gaze annotations.jpg" src="conference-meeting-notes/miccai'24/images/weaklysupervised-segmentation-using-gaze-annotations.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis" dir="auto" class="heading" id="Concept-Attention_Whitening_for_Interpretable_Skin_Lesion_Diagnosis"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis</h2><div class="heading-children"><div><p dir="auto">The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. Existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. However, a medical image usually contains multiple concepts, and the fine-grained concept annotations are difficult to acquire. In this paper, we aim to interpret representations in deep neural networks by aligning the axes of the latent space with known concepts of interest. We propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train a convolutional neural network (CNN) with an inserted CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, the orthogonal matrix is calculated under the guidance of the concept attention mask. We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1272_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1272_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/147-Paper1272.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/147-Paper1272.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Concept Attention Whitening for Interpretable Skin Lesion Diagnosis.jpg" src="Concept Attention Whitening for Interpretable Skin Lesion Diagnosis.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Concept Attention Whitening for Interpretable Skin Lesion Diagnosis.jpg" src="conference-meeting-notes/miccai'24/images/concept-attention-whitening-for-interpretable-skin-lesion-diagnosis.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Baikal: Unpaired Denoising of Fluorescence Microscopy Images using Diffusion Models" dir="auto" class="heading" id="Baikal:_Unpaired_Denoising_of_Fluorescence_Microscopy_Images_using_Diffusion_Models"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Baikal: Unpaired Denoising of Fluorescence Microscopy Images using Diffusion Models</h2><div class="heading-children"><div><p dir="auto">Fluorescence microscopy is an indispensable tool for biological discovery but image quality is constrained by desired spatial and temporal resolution, sample sensitivity, and other factors. Computational denoising methods can bypass imaging constraints and improve signal-to-noise ratio in images. However, current state of the art methods are commonly trained in a supervised manner, requiring paired noisy and clean images, limiting their application across diverse datasets. An alternative class of denoising models can be trained in a self-supervised manner, assuming independent noise across samples but are unable to generalize from available unpaired clean images. A method that can be trained without paired data and can use information from available unpaired high-quality images would address both weaknesses. Here, we present Baikal, a first attempt to formulate such a framework using Denoising Diffusion Probabilistic Models (DDPM) for fluorescence microscopy images. We first train a DDPM backbone in an unconditional manner to learn generative priors over complex morphologies in microscopy images, we can then apply various conditioning strategies to sample from the trained model and propose optimal strategy to denoise the desired image. Extensive quantitative comparisons demonstrate better performance of Baikal over state of the art self-supervised methods across multiple datasets. We highlight the advantage of generative priors learnt by DDPMs in denoising complex Flywing morphologies where other methods fail. Overall, our DDPM based denoising framework presents a new class of denoising method for fluorescence microscopy datasets that achieve good performance without collection of paired high-quality images.  </p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3885_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3885_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/093-Paper3885.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/093-Paper3885.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/scelesticsiva/denoising" rel="noopener" class="external-link" href="https://github.com/scelesticsiva/denoising" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Baikal.jpg" src="Baikal.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Baikal.jpg" src="conference-meeting-notes/miccai'24/images/baikal.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Curriculum Prompting Foundation Models for Medical Image Segmentation" dir="auto" class="heading" id="Curriculum_Prompting_Foundation_Models_for_Medical_Image_Segmentation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Curriculum Prompting Foundation Models for Medical Image Segmentation</h2><div class="heading-children"><div><p dir="auto">Adapting large pre-trained foundation models, e.g., SAM, for medical image segmentation remains a significant challenge. A crucial step involves the formulation of a series of specialized prompts that incorporate specific clinical instructions. Past works have been heavily reliant on a singular type of prompt for each instance, necessitating manual input of an ideally correct prompt, which is less efficient. To tackle this issue, we propose to utilize prompts of different granularity, which are sourced from original images to provide a broader scope of clinical insights. However, combining prompts of varying types can pose a challenge due to potential conflicts. In response, we have designed a coarse-to-fine mechanism, referred to as curriculum prompting, that progressively integrates prompts of different types. Through extensive experiments on three public medical datasets across various modalities, we demonstrate the effectiveness of our proposed approach, which not only automates the prompt generation process but also yields superior performance compared to other SAM-based medical image segmentation methods. Code will be available at: <a rel="noopener" class="external-link" href="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" target="_blank">https://github.com/AnnaZzz-zxq/Curriculum-Prompting</a>.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2832_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2832_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/183-Paper2832.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/183-Paper2832.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" rel="noopener" class="external-link" href="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Cirriculum Prompting for Segmentation.jpg" src="Cirriculum Prompting for Segmentation.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Cirriculum Prompting for Segmentation.jpg" src="conference-meeting-notes/miccai'24/images/cirriculum-prompting-for-segmentation.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations" dir="auto" class="heading" id="NODER:_Image_Sequence_Regression_Based_on_Neural_Ordinary_Differential_Equations"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations</h2><div class="heading-children"><div><p dir="auto">Regression on medical image sequences can capture temporal image pattern changes and predict images at missing or future time points. However, existing geodesic regression methods limit their regression performance by a strong underlying assumption of linear dynamics, while diffusion-based methods have high computational costs and lack constraints to preserve image topology. In this paper, we propose an optimization-based new framework called NODER, which leverages neural ordinary differential equations to capture complex underlying dynamics and reduces its high computational cost of handling high-dimensional image volumes by introducing the latent space. We compare our NODER with two recent regression methods, and the experimental results on ADNI and ACDC datasets demonstrate that our method achieves the SOTA performance in 3D image regression. Our model needs only a couple of images in a sequence for prediction, which is practical, especially for clinical situations where extremely limited image time series are available for analysis.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1899_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1899_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/563-Paper1899.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/563-Paper1899.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/ZedKing12138/NODER-pytorch" rel="noopener" class="external-link" href="https://github.com/ZedKing12138/NODER-pytorch" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Noder.jpg" src="Noder.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Noder.jpg" src="conference-meeting-notes/miccai'24/images/noder.jpg"></span></p></div></div></div><div class="heading-wrapper"><h1 data-heading="Max" dir="auto" class="heading" id="Max">Max</h1><div class="heading-children"><div class="heading-wrapper"><h2 data-heading="DermaVQA: A Multilingual Visual Question Answering Dataset for Dermatology" dir="auto" class="heading" id="DermaVQA:_A_Multilingual_Visual_Question_Answering_Dataset_for_Dermatology"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>DermaVQA: A Multilingual Visual Question Answering Dataset for Dermatology</h2><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/211-Paper2444.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/211-Paper2444.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/velvinnn/DermaVQA" rel="noopener" class="external-link" href="https://github.com/velvinnn/DermaVQA" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="DermaVQA.jpg" src="DermaVQA.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="DermaVQA.jpg" src="conference-meeting-notes/miccai'24/images/dermavqa.jpg"></span></p></div><div><p dir="auto"><span alt="Spot the Difference.jpg" src="Spot the Difference.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Spot the Difference.jpg" src="conference-meeting-notes/miccai'24/images/spot-the-difference.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection" dir="auto" class="heading" id="MediCLIP:_Adapting_CLIP_for_Few-shot_Medical_Image_Anomaly_Detection"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection</h2><div class="heading-children"><div><p dir="auto">In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians. However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost. This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the medical field where data collection and annotation are both very expensive. We propose an innovative approach, MediCLIP, which adapts the CLIP model to few-shot medical image anomaly detection through self-supervised fine-tuning. Although CLIP, as a vision-language model, demonstrates outstanding zero-/few-shot performance on various downstream tasks, it still falls short in the anomaly detection of medical images. To address this, we design a series of medical image anomaly synthesis tasks to simulate common disease patterns in medical imaging, transferring the powerful generalization capabilities of CLIP to the task of medical image anomaly detection. When only few-shot normal medical images are provided, MediCLIP achieves state-of-the-art performance in anomaly detection and location compared to other methods. Extensive experiments on three distinct medical anomaly detection tasks have demonstrated the superiority of our approach. The code is available at <a rel="noopener" class="external-link" href="https://github.com/cnulab/MediCLIP" target="_blank">https://github.com/cnulab/MediCLIP</a>.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/0333_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/0333_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/504-Paper0333.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/504-Paper0333.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/cnulab/MediCLIP" rel="noopener" class="external-link" href="https://github.com/cnulab/MediCLIP" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Mediclip.jpg" src="Mediclip.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Mediclip.jpg" src="conference-meeting-notes/miccai'24/images/mediclip.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Spot the Difference: Difference Visual Question Answering with Residual Alignment" dir="auto" class="heading" id="Spot_the_Difference:_Difference_Visual_Question_Answering_with_Residual_Alignment"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Spot the Difference: Difference Visual Question Answering with Residual Alignment</h2><div class="heading-children"><div><p dir="auto">Difference Visual Question Answering (DiffVQA) introduces a new task aimed at understanding and responding to questions regarding the disparities observed between two images. Unlike traditional medical VQA tasks, DiffVQA closely mirrors the diagnostic procedures of radiologists, who frequently conduct longitudinal comparisons of images taken at different time points for a given patient. This task accentuates the discrepancies between images captured at distinct temporal intervals.To better address the variations, this paper proposes a novel Residual Alignment model (ReAl) tailored for DiffVQA. ReAl is designed to produce flexible and accurate answers by analyzing the discrepancies in chest X-ray images of the same patient across different time points. Compared to the previous method, ReAl additionally aid a residual input branch, where the residual of two images is fed into this branch. Additionally, a Residual Feature Alignment (RFA) module is introduced to ensure that ReAl effectively captures and learns the disparities between corresponding images. Experimental evaluations conducted on the MIMIC-Diff-VQA dataset demonstrate the superiority of ReAl over previous state-of-the-art methods, consistently achieving better performance. Ablation experiments further validate the effectiveness of the RFA module in enhancing the model’s attention to differences. The code implementation of the proposed approach will be made available.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2957_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2957_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/726-Paper2957.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/726-Paper2957.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="" target="_blank">Code</a></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Semi-Supervised Contrastive VAE for Disentanglement of Digital Pathology Images" dir="auto" class="heading" id="Semi-Supervised_Contrastive_VAE_for_Disentanglement_of_Digital_Pathology_Images"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Semi-Supervised Contrastive VAE for Disentanglement of Digital Pathology Images</h2><div class="heading-children"><div><p dir="auto">Despite the strong prediction power of deep learning models, their interpretability remains an important concern. Disentanglement models increase interpretability by decomposing the latent space into interpretable subspaces. In this paper, we propose the first disentanglement method for pathology images. We focus on the task of detecting tumor-infiltrating lymphocytes (TIL). We propose different ideas including cascading disentanglement, novel architecture and reconstruction branches. We achieve superior performance on complex pathology images, thus improving the interpretability and even generalization power of TIL detection deep learning models.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3843_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3843_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/688-Paper3843.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/688-Paper3843.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/Shauqi/SS-cVAE" rel="noopener" class="external-link" href="https://github.com/Shauqi/SS-cVAE" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="ss-cvae.jpg" src="ss-cvae.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="ss-cvae.jpg" src="conference-meeting-notes/miccai'24/images/ss-cvae.jpg"></span></p></div></div></div></div></div><div class="heading-wrapper"><h1 data-heading="Mark" dir="auto" class="heading" id="Mark">Mark</h1><div class="heading-children"><div class="heading-wrapper"><h2 data-heading="EndoSelf: Self-Supervised Monocular 3D Scene Reconstruction of Deformable Tissues with Neural Radiance Fields on Endoscopic Videos" dir="auto" class="heading" id="EndoSelf:_Self-Supervised_Monocular_3D_Scene_Reconstruction_of_Deformable_Tissues_with_Neural_Radiance_Fields_on_Endoscopic_Videos"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>EndoSelf: Self-Supervised Monocular 3D Scene Reconstruction of Deformable Tissues with Neural Radiance Fields on Endoscopic Videos</h2><div class="heading-children"><div><p dir="auto">Neural radiance field has recently emerged as a powerful representation to reconstruct deformable tissues from endoscopic videos. Previous methods mainly focus on depth-supervised approaches based on endoscopic datasets. As additional information, depth values were proven important in reconstructing deformable tissues by previous methods. However, collecting a large number of datasets with accurate depth values limits the applicability of these approaches for endoscopic scenes. To address this issue, we propose a novel self-supervised monocular 3D scene reconstruction method based on neural radiance fields without prior depth as supervision. We consider the monocular 3D reconstruction based on two approaches: ray-tracing-based neural radiance fields and structure-from-motion-based photogrammetry. We introduce structure from motion framework and leverage color values as a supervision to complete the self-supervised learning strategy. In addition, we predict the depth values from neural radiance fields and enforce the geometric constraint for depth values from adjacent views. Moreover, we propose a looped loss function to fully explore the temporal correlation between input images. The experimental results showed that the proposed method without prior depth outperformed the previous depth-supervised methods on two endoscopic datasets. Our code is available.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1656_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1656_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/276-Paper1656.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/276-Paper1656.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/MoriLabNU/EndoSelf" rel="noopener" class="external-link" href="https://github.com/MoriLabNU/EndoSelf" target="_blank">Code</a><br>
<span alt="EndoSelf.jpg" src="EndoSelf.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="EndoSelf.jpg" src="conference-meeting-notes/miccai'24/images/endoself.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Misaligned 3D Texture Optimization in MIS Utilizing Generative Framework" dir="auto" class="heading" id="Misaligned_3D_Texture_Optimization_in_MIS_Utilizing_Generative_Framework"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Misaligned 3D Texture Optimization in MIS Utilizing Generative Framework</h2><div class="heading-children"><div><p dir="auto">Three-dimensional reconstruction of the surgical area based on intraoperative laparoscopic videos can restore 2D information to 3D space, providing a solid technical foundation for many applications in computer-assisted surgery. SLAM methods often suffer from imperfect pose estimation and tissue motion, leading to the loss of original texture information. On the other hand, methods like Neural Radiance Fields and 3D Gaussian Split require offline processing and lack generalization capabilities. To overcome these limitations, we explore a texture optimization method that generates high resolution and continuous texture. It designs a mechanism for transforming 3D point clouds into 2D texture space and utilizes a generative network architecture to design 2D registration and image fusion modules. Experimental results and comparisons with state-of-the-art techniques demonstrate the effectiveness of this method in preserving the high-fidelity texture.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2885_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2885_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/518-Paper2885.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/518-Paper2885.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="" target="_blank">Code</a><br>
<span alt="Misaligned3D.jpg" src="Misaligned3D.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Misaligned3D.jpg" src="conference-meeting-notes/miccai'24/images/misaligned3d.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy" dir="auto" class="heading" id="MeshBrush:_Painting_the_Anatomical_Mesh_with_Neural_Stylization_for_Endoscopy"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy</h2><div class="heading-children"><div><p dir="auto">Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering synthetic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate structurally accurate simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN can imitate realistic endoscopic images from these simulations, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training networks and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures. The code will be made public on GitHub.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/510-Paper2259.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/510-Paper2259.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Meshbrush.jpg" src="Meshbrush.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Meshbrush.jpg" src="conference-meeting-notes/miccai'24/images/meshbrush.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction" dir="auto" class="heading" id="SurgicalGaussian:_Deformable_3D_Gaussians_for_High-Fidelity_Surgical_Scene_Reconstruction"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction</h2><div class="heading-children"><div><p dir="auto">Dynamic reconstruction of deformable tissues in endoscopic video is a key technology for robot-assisted surgery. Recent reconstruction methods based on neural radiance fields (NeRFs) have achieved remarkable results in the reconstruction of surgical scenes. However, based on implicit representation, NeRFs struggle to capture the intricate details of objects in the scene and cannot achieve real-time rendering. In addition, restricted single view perception and occluded instruments also propose special challenges in surgical scene reconstruction. To address these issues, we develop SurgicalGaussian, a deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our approach models the spatio-temporal features of soft tissues at each time stamp via a forward-mapping deformation MLP and regularization to constrain local 3D Gaussians to comply with consistent movement. With the depth initialization strategy and tool mask-guided training, our method can remove surgical instruments and reconstruct high-fidelity surgical scenes. Through experiments on various surgical videos, our network outperforms existing method on many aspects, including rendering quality, rendering speed and GPU usage. The project page can be found at <a rel="noopener" class="external-link" href="https://surgicalgaussian.github.io" target="_blank">https://surgicalgaussian.github.io</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3077_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3077_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/745-Paper3077.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/745-Paper3077.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://surgicalgaussian.github.io." rel="noopener" class="external-link" href="https://surgicalgaussian.github.io." target="_blank">Code</a></p></div><div><p dir="auto"><span alt="SurgicalGaussian.jpg" src="SurgicalGaussian.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="SurgicalGaussian.jpg" src="conference-meeting-notes/miccai'24/images/surgicalgaussian.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Topological SLAM in colonoscopies leveraging deep features and topological priors" dir="auto" class="heading" id="Topological_SLAM_in_colonoscopies_leveraging_deep_features_and_topological_priors"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Topological SLAM in colonoscopies leveraging deep features and topological priors</h2><div class="heading-children"><div><p dir="auto">We introduce ColonSLAM, a system that combines classical multiple-map metric SLAM with deep features and topological priors to create topological maps of the whole colon. The SLAM pipeline by itself is able to create disconnected individual metric submaps representing locations from short video subsections of the colon, but is not able to merge covisible submaps due to deformations and the limited performance of the SIFT descriptor in the medical domain. ColonSLAM is guided by topological priors and combines a deep localization network trained to distinguish if two images come from the same place or not and the soft verification of a transformer-based matching network, being able to relate far-in-time submaps during an exploration, grouping them in nodes imaging the same colon place, building more complex maps than any other approach in the literature. We demonstrate our approach in the Endomapper dataset, showing its potential for producing maps of the whole colon in real human explorations. Code and models are available at: <a rel="noopener" class="external-link" href="https://github.com/endomapper/ColonSLAM" target="_blank">https://github.com/endomapper/ColonSLAM</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1110_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1110_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/778-Paper1110.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/778-Paper1110.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/endomapper/ColonSLAM" rel="noopener" class="external-link" href="https://github.com/endomapper/ColonSLAM" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Topological Slam.jpg" src="Topological Slam.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Topological Slam.jpg" src="conference-meeting-notes/miccai'24/images/topological-slam.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Trans-Window Panoramic Impasto for Online Tissue Deformation Recovery" dir="auto" class="heading" id="Trans-Window_Panoramic_Impasto_for_Online_Tissue_Deformation_Recovery"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Trans-Window Panoramic Impasto for Online Tissue Deformation Recovery</h2><div class="heading-children"><div><p dir="auto">Deformation recovery from laparoscopic images benefits many downstream applications like robot planning, intraoperative navigation and surgical safety assessment. We define tissue deformation as time-variant surface structure and displacement. Besides, we also pay attention to the surface strain, which bridges the visual observation and the tissue biomechanical status, for which continuous pointwise surface mapping and tracking are necessary. Previous SLAM-based methods cannot cope with instrument-induced occlusion and severe scene deformation, while the neural field-based ones are offline and scene-specific, which hinders their application in continuous mapping. Moreover, neither approach meets the requirement of continuous pointwise tracking. To overcome these limitations, we assume a deformable environment and a movable window through which an observer depicts the environment’s 3D structure on a canonical canvas as maps in a process named impasto. The observer performs panoramic impasto for the currently and previously observed 3D structure in a two-step online approach: optimization and fusion. The optimization of the maps compensates for the error in the observation of the structure and the tracking by preserving spatiotemporal smoothness, while the fusion is for merging the estimated and the newly observed maps by ensuring visibility. Experiments were conducted using ex vivo and in vivo stereo laparoscopic datasets where tool-tissue interaction occurs and large camera motion exists. Results demonstrate that the proposed online method is robust to instrument-induced occlusion, capable of estimating surface strain, and can continuously reconstruct and track surface points regardless of camera motion. Code is available at: <a rel="noopener" class="external-link" href="https://github.com/bmpelab/trans_window_panoramic_impasto.git" target="_blank">https://github.com/bmpelab/trans_window_panoramic_impasto.git</a></p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/4075_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/4075_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/800-Paper4075.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/800-Paper4075.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/bmpelab/trans_window_panoramic_impasto.git" rel="noopener" class="external-link" href="https://github.com/bmpelab/trans_window_panoramic_impasto.git" target="_blank">Code</a></p></div><div><p dir="auto"><span alt="Transwindow.jpg" src="Transwindow.jpg" class="internal-embed media-embed image-embed is-loaded"><img alt="Transwindow.jpg" src="conference-meeting-notes/miccai'24/images/transwindow.jpg"></span></p></div></div></div><div class="heading-wrapper"><h2 data-heading="Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction" dir="auto" class="heading" id="Gaussian_Pancakes:_Geometrically-Regularized_3D_Gaussian_Splatting_for_Realistic_Endoscopic_Reconstruction"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction</h2><div class="heading-children"><div><p dir="auto">Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of pre- cancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce “Gaussian Pancakes”, a method that lever- ages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100× faster rendering and more than 10× shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.</p></div><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2298_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2298_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/349-Paper2298.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/349-Paper2298.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/smbonilla/GaussianPancakes" rel="noopener" class="external-link" href="https://github.com/smbonilla/GaussianPancakes" target="_blank">Code</a></p></div><div class="mod-footer"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#MICCAI'24 Recap"><div class="tree-item-contents heading-link" heading-name="MICCAI'24 Recap"><span class="tree-item-title">MICCAI'24 Recap</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#A_framework_for_assessing_joint_human-AI_systems_based_on_uncertainty_estimation"><div class="tree-item-contents heading-link" heading-name="A framework for assessing joint human-AI systems based on uncertainty estimation"><span class="tree-item-title">A framework for assessing joint human-AI systems based on uncertainty estimation</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Fair_and_Accurate_Skin_Disease_Image_Classification_by_Alignment_with_Clinical_Labels"><div class="tree-item-contents heading-link" heading-name="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels"><span class="tree-item-title">Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#FairQuantize:_Achieving_Fairness_Through_Weight_Quantization_for_Dermatological_Disease_Diagnosis"><div class="tree-item-contents heading-link" heading-name="FairQuantize: Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis"><span class="tree-item-title">FairQuantize: Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#SkinCON:_Towards_consensus_for_the_uncertainty_of_skin_cancer_sub-typing_through_distribution_regularized_adaptive_predictive_sets_(DRAPS)"><div class="tree-item-contents heading-link" heading-name="SkinCON: Towards consensus for the uncertainty of skin cancer sub-typing through distribution regularized adaptive predictive sets (DRAPS)"><span class="tree-item-title">SkinCON: Towards consensus for the uncertainty of skin cancer sub-typing through distribution regularized adaptive predictive sets (DRAPS)</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#MedSynth:_Leveraging_Generative_Model_for_Healthcare_Data_Sharing"><div class="tree-item-contents heading-link" heading-name="MedSynth: Leveraging Generative Model for Healthcare Data Sharing"><span class="tree-item-title">MedSynth: Leveraging Generative Model for Healthcare Data Sharing</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#S-SYNTH:_Knowledge-Based,_Synthetic_Generation_of_Skin_Images"><div class="tree-item-contents heading-link" heading-name="S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images"><span class="tree-item-title">S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#AdaCBM:_An_Adaptive_Concept_Bottleneck_Model_for_Explainable_and_Accurate_Diagnosis"><div class="tree-item-contents heading-link" heading-name="AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis"><span class="tree-item-title">AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Weakly-supervised_Medical_Image_Segmentation_with_Gaze_Annotations"><div class="tree-item-contents heading-link" heading-name="Weakly-supervised Medical Image Segmentation with Gaze Annotations"><span class="tree-item-title">Weakly-supervised Medical Image Segmentation with Gaze Annotations</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Concept-Attention_Whitening_for_Interpretable_Skin_Lesion_Diagnosis"><div class="tree-item-contents heading-link" heading-name="Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis"><span class="tree-item-title">Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Baikal:_Unpaired_Denoising_of_Fluorescence_Microscopy_Images_using_Diffusion_Models"><div class="tree-item-contents heading-link" heading-name="Baikal: Unpaired Denoising of Fluorescence Microscopy Images using Diffusion Models"><span class="tree-item-title">Baikal: Unpaired Denoising of Fluorescence Microscopy Images using Diffusion Models</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Curriculum_Prompting_Foundation_Models_for_Medical_Image_Segmentation"><div class="tree-item-contents heading-link" heading-name="Curriculum Prompting Foundation Models for Medical Image Segmentation"><span class="tree-item-title">Curriculum Prompting Foundation Models for Medical Image Segmentation</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#NODER:_Image_Sequence_Regression_Based_on_Neural_Ordinary_Differential_Equations"><div class="tree-item-contents heading-link" heading-name="NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations"><span class="tree-item-title">NODER: Image Sequence Regression Based on Neural Ordinary Differential Equations</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Max"><div class="tree-item-contents heading-link" heading-name="Max"><span class="tree-item-title">Max</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#DermaVQA:_A_Multilingual_Visual_Question_Answering_Dataset_for_Dermatology"><div class="tree-item-contents heading-link" heading-name="DermaVQA: A Multilingual Visual Question Answering Dataset for Dermatology"><span class="tree-item-title">DermaVQA: A Multilingual Visual Question Answering Dataset for Dermatology</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#MediCLIP:_Adapting_CLIP_for_Few-shot_Medical_Image_Anomaly_Detection"><div class="tree-item-contents heading-link" heading-name="MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection"><span class="tree-item-title">MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Spot_the_Difference:_Difference_Visual_Question_Answering_with_Residual_Alignment"><div class="tree-item-contents heading-link" heading-name="Spot the Difference: Difference Visual Question Answering with Residual Alignment"><span class="tree-item-title">Spot the Difference: Difference Visual Question Answering with Residual Alignment</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Semi-Supervised_Contrastive_VAE_for_Disentanglement_of_Digital_Pathology_Images"><div class="tree-item-contents heading-link" heading-name="Semi-Supervised Contrastive VAE for Disentanglement of Digital Pathology Images"><span class="tree-item-title">Semi-Supervised Contrastive VAE for Disentanglement of Digital Pathology Images</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Mark"><div class="tree-item-contents heading-link" heading-name="Mark"><span class="tree-item-title">Mark</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#EndoSelf:_Self-Supervised_Monocular_3D_Scene_Reconstruction_of_Deformable_Tissues_with_Neural_Radiance_Fields_on_Endoscopic_Videos"><div class="tree-item-contents heading-link" heading-name="EndoSelf: Self-Supervised Monocular 3D Scene Reconstruction of Deformable Tissues with Neural Radiance Fields on Endoscopic Videos"><span class="tree-item-title">EndoSelf: Self-Supervised Monocular 3D Scene Reconstruction of Deformable Tissues with Neural Radiance Fields on Endoscopic Videos</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Misaligned_3D_Texture_Optimization_in_MIS_Utilizing_Generative_Framework"><div class="tree-item-contents heading-link" heading-name="Misaligned 3D Texture Optimization in MIS Utilizing Generative Framework"><span class="tree-item-title">Misaligned 3D Texture Optimization in MIS Utilizing Generative Framework</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#MeshBrush:_Painting_the_Anatomical_Mesh_with_Neural_Stylization_for_Endoscopy"><div class="tree-item-contents heading-link" heading-name="MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy"><span class="tree-item-title">MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#SurgicalGaussian:_Deformable_3D_Gaussians_for_High-Fidelity_Surgical_Scene_Reconstruction"><div class="tree-item-contents heading-link" heading-name="SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction"><span class="tree-item-title">SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Topological_SLAM_in_colonoscopies_leveraging_deep_features_and_topological_priors"><div class="tree-item-contents heading-link" heading-name="Topological SLAM in colonoscopies leveraging deep features and topological priors"><span class="tree-item-title">Topological SLAM in colonoscopies leveraging deep features and topological priors</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Trans-Window_Panoramic_Impasto_for_Online_Tissue_Deformation_Recovery"><div class="tree-item-contents heading-link" heading-name="Trans-Window Panoramic Impasto for Online Tissue Deformation Recovery"><span class="tree-item-title">Trans-Window Panoramic Impasto for Online Tissue Deformation Recovery</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="conference-meeting-notes/miccai'24/miccai'24-recap.html#Gaussian_Pancakes:_Geometrically-Regularized_3D_Gaussian_Splatting_for_Realistic_Endoscopic_Reconstruction"><div class="tree-item-contents heading-link" heading-name="Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction"><span class="tree-item-title">Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>