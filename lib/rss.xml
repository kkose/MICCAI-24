<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>Notes</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 16 Oct 2024 13:38:11 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 16 Oct 2024 13:38:11 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[MICCAI'24 Recap]]></title><description><![CDATA[ 
 <br><br>We investigate the role of uncertainty quantification in aiding medical decision-making. Existing evaluation metrics fail to capture the practical utility of joint human-AI decision-making systems. To address this, we introduce a novel framework to assess such systems and use it to benchmark a diverse set of confidence and uncertainty estimation methods. Our results show that certainty measures enable joint human-AI systems to outperform both standalone humans and AIs, and that for a given system there exists an optimal balance in the number of cases to refer to humans, beyond which the system’s performance degrades.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1535_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1535_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/016-Paper1535.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/016-Paper1535.html" target="_blank">Full Review</a><br><img alt="A framework for assessing joint human-AI systems based on uncertainty estimation.jpg" src="/conference-meeting-notes/miccai'24/images/a-framework-for-assessing-joint-human-ai-systems-based-on-uncertainty-estimation.jpg"><br><br>Deep learning models have achieved great success in automating skin lesion diagnosis. However, the ethnic disparity in these models’ predictions needs to be addressed before deployment of these models. We introduce a novel approach: PatchAlign, to enhance skin condition image classification accuracy and fairness through alignment with clinical text representations of skin conditions. PatchAlign uses Graph Optimal Transport (\texttt{GOT}) Loss as a regularizer to perform cross-domain alignment. The representations thus obtained are robust and generalize well across skin tones, even with limited training samples. To reduce the effect of noise/artifacts in clinical dermatology images, we propose a learnable Masked Graph Optimal Transport for cross-domain alignment that further improves the fairness metrics.<br>We compare our model to the SOTA model (FairDisCo) on two skin lesion datasets with different skin types: Fitzpatrick17k and Diverse Dermatology Images (DDI). Our proposed approach, PatchAlign, enhances the accuracy of skin condition image classification by 2.8% (in-domain) and 6.2% (out-domain) on Fitzpatrick17k and 4.2% (in-domain) on DDI compared to FairDisCo. In addition, it consistently improves the fairness of true positive rates across skin tones in all of our experiments.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2609_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2609_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/308-Paper2609.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/308-Paper2609.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/aayushmanace/PatchAlign24" rel="noopener" class="external-link" href="https://github.com/aayushmanace/PatchAlign24" target="_blank">Code</a><br><img alt="Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels.jpg" src="/conference-meeting-notes/miccai'24/images/fair-and-accurate-skin-disease-image-classification-by-alignment-with-clinical-labels.jpg"><br><br>Recent studies have demonstrated that deep learning (DL) models for medical image classification may exhibit biases toward certain demographic attributes such as race, gender, and age. Existing bias mitigation strategies often require sensitive attributes for inference, which may not always be available, or achieve moderate fairness enhancement at the cost of significant accuracy decline. To overcome these obstacles, we propose FairQuantize, a novel approach that ensures fairness by quantizing model weights. We reveal that quantization can be used not as a tool for model compression but as a means to improve model fairness. It is based on the observation that different weights in a model impact performance on various demographic groups differently. FairQuantize selectively quantizes certain weights to enhance fairness while only marginally impacting accuracy. In addition, resulting quantized models can work without sensitive attributes as input. Experimental results on two skin disease datasets demonstrate that FairQuantize can significantly enhance fairness among sensitive attributes while minimizing the impact on overall performance.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3697_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3697_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/310-Paper3697.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/310-Paper3697.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/guoyb17/FairQuantize" rel="noopener" class="external-link" href="https://github.com/guoyb17/FairQuantize" target="_blank">Code</a><br>
<img alt="FairQuantize Achieving Fairness Through Weight Quantization for Dermatological Disease Diagnosis.jpg" src="/conference-meeting-notes/miccai'24/images/fairquantize-achieving-fairness-through-weight-quantization-for-dermatological-disease-diagnosis.jpg"><br><br>Deep learning has been widely utilized in medical diagnosis. Convolutional neural networks and transformers can achieve high predictive accuracy, which can be on par with or even exceed human performance. However, uncertainty quantification remains an unresolved issue, impeding the deployment of deep learning models in practical settings. Conformal analysis can, in principle, estimate the uncertainty of each diagnostic prediction, but doing so effectively requires extensive human annotations to characterize the underlying empirical distributions. This has been challenging in the past because instance-level class distribution data has been unavailable: Collecting massive ground truth labels is already challenging, and obtaining the class distribution of each instance is even more difficult. Here, we provide a large skin cancer instance-level class distribution dataset, SkinCON, that contains 25,331 skin cancer images from the ISIC 2019 challenge dataset. SkinCON is built upon over 937,167 diagnostic judgments from 10,509 participants. Using SkinCON, we propose the distribution regularized adaptive predictive sets (DRAPS) method for skin cancer diagnosis. We also provide a new evaluation metric based on SkinCON. Experiment results show the quality of our proposed DRAPS method and the uncertainty variation with respect to patient age and sex from health equity and fairness perspective. The dataset and code are available at <a data-tooltip-position="top" aria-label="https://skincon.github.io." rel="noopener" class="external-link" href="https://skincon.github.io." target="_blank">https://skincon.github.io</a><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/4090_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/4090_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/708-Paper4090.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/708-Paper4090.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://skincon.github.io." rel="noopener" class="external-link" href="https://skincon.github.io." target="_blank">Code</a><br><img alt="SkinCon.jpg" src="/conference-meeting-notes/miccai'24/images/skincon.jpg"><br><br>Sharing medical datasets among healthcare organizations is essential for advancing AI-assisted disease diagnostics and enhancing patient care. Employing techniques like data de-identification and data synthesis in medical data sharing, however, comes with inherent drawbacks that may lead to privacy leakage. Therefore, there is a pressing need for mechanisms that can effectively conceal sensitive information, ensuring a secure environment for data sharing. Dataset Condensation (DC) emerges as a solution, creating a reduced-scale synthetic dataset from a larger original dataset while maintaining comparable training outcomes. This approach offers advantages in terms of privacy and communication efficiency in the context of medical data sharing. Despite these benefits, traditional condensation methods encounter challenges, particularly with high-resolution medical datasets. To address these challenges, we present MedSynth, a novel dataset condensation scheme designed to efficiently condense the knowledge within extensive medical datasets into a generative model. This facilitates the sharing of the generative model across hospitals without the need to disclose raw data. By combining an attention-based generator with a vision transformer (ViT), MedSynth creates a generative model capable of producing a concise set of representative synthetic medical images, encapsulating the features of the original dataset. This generative model can then be shared with hospitals to optimize various downstream model training tasks. Extensive experimental results across medical datasets demonstrate that MedSynth outperforms state-of-the-art methods. Moreover, MedSynth successfully defends against state-of-the-art Membership Inference Attacks (MIA), highlighting its significant potential in preserving the privacy of medical data.  <br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2872_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2872_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/506-Paper2872.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/506-Paper2872.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="." target="_blank">Code</a><br><img alt="MedSynth.jpg" src="/conference-meeting-notes/miccai'24/images/medsynth.jpg"><br><br>Development of artificial intelligence (AI) techniques in medical imaging requires access to large-scale and diverse datasets for training and evaluation. In dermatology, obtaining such datasets remains challenging due to significant variations in patient populations, illumination conditions, and acquisition system characteristics. In this work, we propose S-SYNTH, the first knowledge-based, adaptable open-source skin simulation framework to rapidly generate synthetic skin, 3D models and digitally rendered images, using an anatomically inspired multi-layer, multi-component skin and growing lesion model. The skin model allows for controlled variation in skin appearance, such as skin color, presence of hair, lesion shape, and blood fraction among other parameters. We use this framework to study the effect of possible variations on the development and evaluation of AI models for skin lesion segmentation, and show that results obtained using synthetic data follow similar comparative trends as real dermatologic images, while mitigating biases and limitations from existing datasets including small dataset size, lack of diversity, and underrepresentation.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1426_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1426_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/729-Paper1426.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/729-Paper1426.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/DIDSR/ssynth-release" rel="noopener" class="external-link" href="https://github.com/DIDSR/ssynth-release" target="_blank">Code</a><br><img alt="S-Synth.jpg" src="/conference-meeting-notes/miccai'24/images/s-synth.jpg"><br><br>The integration of vision-language models such as CLIP and Concept Bottleneck Models (CBMs) offers a promising approach to explaining deep neural network (DNN) decisions using concepts understandable by humans, addressing the black-box concern of DNNs. While CLIP provides both explainability and zero-shot classification capability, its pre-training on generic image and text data may limit its classification accuracy and applicability to medical image diagnostic tasks, creating a transfer learning problem. To maintain explainability and address transfer learning needs, CBM methods commonly design post-processing modules after the bottleneck module. However, this way has been ineffective. This paper takes an unconventional approach by re-examining the CBM framework through the lens of its geometrical representation as a simple linear classification system. The analysis uncovers that post-CBM fine-tuning modules merely rescale and shift the classification outcome of the system, failing to fully leverage the system’s learning potential. We introduce an adaptive module strategically positioned between CLIP and CBM to bridge the gap between source and downstream domains. This simple yet effective approach enhances classification performance while preserving the explainability afforded by the framework. Our work offers a comprehensive solution that encompasses the entire process, from concept discovery to model training, providing a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3895_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3895_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/045-Paper3895.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/045-Paper3895.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="%5Bhttps://github.com/AIML-MED/AdaCBM%5D(https://github.com/AIML-MED/AdaCBM)" rel="noopener" class="external-link" href="/[https://github.com/AIML-MED/AdaCBM](https://github.com/AIML-MED/AdaCBM)" target="_blank">Code</a><br><img alt="AdaCBM.jpg" src="/conference-meeting-notes/miccai'24/images/adacbm.jpg"><br><br>Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at: <a rel="noopener" class="external-link" href="https://github.com/med-air/GazeMedSeg" target="_blank">https://github.com/med-air/GazeMedSeg</a><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1675_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1675_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/843-Paper1675.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/843-Paper1675.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/med-air/GazeMedSeg" rel="noopener" class="external-link" href="https://github.com/med-air/GazeMedSeg" target="_blank">Code</a><br><img alt="Weaklysupervised segmentation using gaze annotations.jpg" src="/conference-meeting-notes/miccai'24/images/weaklysupervised-segmentation-using-gaze-annotations.jpg"><br><br>The black-box nature of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. Existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. However, a medical image usually contains multiple concepts, and the fine-grained concept annotations are difficult to acquire. In this paper, we aim to interpret representations in deep neural networks by aligning the axes of the latent space with known concepts of interest. We propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train a convolutional neural network (CNN) with an inserted CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, the orthogonal matrix is calculated under the guidance of the concept attention mask. We particularly introduce a weakly-supervised concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1272_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1272_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/147-Paper1272.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/147-Paper1272.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="." target="_blank">Code</a><br><img alt="Concept Attention Whitening for Interpretable Skin Lesion Diagnosis.jpg" src="/conference-meeting-notes/miccai'24/images/concept-attention-whitening-for-interpretable-skin-lesion-diagnosis.jpg"><br><br>Fluorescence microscopy is an indispensable tool for biological discovery but image quality is constrained by desired spatial and temporal resolution, sample sensitivity, and other factors. Computational denoising methods can bypass imaging constraints and improve signal-to-noise ratio in images. However, current state of the art methods are commonly trained in a supervised manner, requiring paired noisy and clean images, limiting their application across diverse datasets. An alternative class of denoising models can be trained in a self-supervised manner, assuming independent noise across samples but are unable to generalize from available unpaired clean images. A method that can be trained without paired data and can use information from available unpaired high-quality images would address both weaknesses. Here, we present Baikal, a first attempt to formulate such a framework using Denoising Diffusion Probabilistic Models (DDPM) for fluorescence microscopy images. We first train a DDPM backbone in an unconditional manner to learn generative priors over complex morphologies in microscopy images, we can then apply various conditioning strategies to sample from the trained model and propose optimal strategy to denoise the desired image. Extensive quantitative comparisons demonstrate better performance of Baikal over state of the art self-supervised methods across multiple datasets. We highlight the advantage of generative priors learnt by DDPMs in denoising complex Flywing morphologies where other methods fail. Overall, our DDPM based denoising framework presents a new class of denoising method for fluorescence microscopy datasets that achieve good performance without collection of paired high-quality images.  <br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3885_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3885_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/093-Paper3885.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/093-Paper3885.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/scelesticsiva/denoising" rel="noopener" class="external-link" href="https://github.com/scelesticsiva/denoising" target="_blank">Code</a><br><img alt="Baikal.jpg" src="/conference-meeting-notes/miccai'24/images/baikal.jpg"><br><br>Adapting large pre-trained foundation models, e.g., SAM, for medical image segmentation remains a significant challenge. A crucial step involves the formulation of a series of specialized prompts that incorporate specific clinical instructions. Past works have been heavily reliant on a singular type of prompt for each instance, necessitating manual input of an ideally correct prompt, which is less efficient. To tackle this issue, we propose to utilize prompts of different granularity, which are sourced from original images to provide a broader scope of clinical insights. However, combining prompts of varying types can pose a challenge due to potential conflicts. In response, we have designed a coarse-to-fine mechanism, referred to as curriculum prompting, that progressively integrates prompts of different types. Through extensive experiments on three public medical datasets across various modalities, we demonstrate the effectiveness of our proposed approach, which not only automates the prompt generation process but also yields superior performance compared to other SAM-based medical image segmentation methods. Code will be available at: <a rel="noopener" class="external-link" href="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" target="_blank">https://github.com/AnnaZzz-zxq/Curriculum-Prompting</a>.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2832_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2832_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/183-Paper2832.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/183-Paper2832.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" rel="noopener" class="external-link" href="https://github.com/AnnaZzz-zxq/Curriculum-Prompting" target="_blank">Code</a><br><img alt="Cirriculum Prompting for Segmentation.jpg" src="/conference-meeting-notes/miccai'24/images/cirriculum-prompting-for-segmentation.jpg"><br><br>Regression on medical image sequences can capture temporal image pattern changes and predict images at missing or future time points. However, existing geodesic regression methods limit their regression performance by a strong underlying assumption of linear dynamics, while diffusion-based methods have high computational costs and lack constraints to preserve image topology. In this paper, we propose an optimization-based new framework called NODER, which leverages neural ordinary differential equations to capture complex underlying dynamics and reduces its high computational cost of handling high-dimensional image volumes by introducing the latent space. We compare our NODER with two recent regression methods, and the experimental results on ADNI and ACDC datasets demonstrate that our method achieves the SOTA performance in 3D image regression. Our model needs only a couple of images in a sequence for prediction, which is practical, especially for clinical situations where extremely limited image time series are available for analysis.<br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1899_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1899_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/563-Paper1899.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/563-Paper1899.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/ZedKing12138/NODER-pytorch" rel="noopener" class="external-link" href="https://github.com/ZedKing12138/NODER-pytorch" target="_blank">Code</a><br><img alt="Noder.jpg" src="/conference-meeting-notes/miccai'24/images/noder.jpg"><br><br><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2444_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/211-Paper2444.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/211-Paper2444.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/velvinnn/DermaVQA" rel="noopener" class="external-link" href="https://github.com/velvinnn/DermaVQA" target="_blank">Code</a><br><img alt="DermaVQA.jpg" src="/conference-meeting-notes/miccai'24/images/dermavqa.jpg"><br><img alt="Spot the Difference.jpg" src="/conference-meeting-notes/miccai'24/images/spot-the-difference.jpg"><br><br>In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians. However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost. This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the medical field where data collection and annotation are both very expensive. We propose an innovative approach, MediCLIP, which adapts the CLIP model to few-shot medical image anomaly detection through self-supervised fine-tuning. Although CLIP, as a vision-language model, demonstrates outstanding zero-/few-shot performance on various downstream tasks, it still falls short in the anomaly detection of medical images. To address this, we design a series of medical image anomaly synthesis tasks to simulate common disease patterns in medical imaging, transferring the powerful generalization capabilities of CLIP to the task of medical image anomaly detection. When only few-shot normal medical images are provided, MediCLIP achieves state-of-the-art performance in anomaly detection and location compared to other methods. Extensive experiments on three distinct medical anomaly detection tasks have demonstrated the superiority of our approach. The code is available at <a rel="noopener" class="external-link" href="https://github.com/cnulab/MediCLIP" target="_blank">https://github.com/cnulab/MediCLIP</a>.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/0333_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/0333_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/504-Paper0333.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/504-Paper0333.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/cnulab/MediCLIP" rel="noopener" class="external-link" href="https://github.com/cnulab/MediCLIP" target="_blank">Code</a><br><img alt="Mediclip.jpg" src="/conference-meeting-notes/miccai'24/images/mediclip.jpg"><br><br>Difference Visual Question Answering (DiffVQA) introduces a new task aimed at understanding and responding to questions regarding the disparities observed between two images. Unlike traditional medical VQA tasks, DiffVQA closely mirrors the diagnostic procedures of radiologists, who frequently conduct longitudinal comparisons of images taken at different time points for a given patient. This task accentuates the discrepancies between images captured at distinct temporal intervals.To better address the variations, this paper proposes a novel Residual Alignment model (ReAl) tailored for DiffVQA. ReAl is designed to produce flexible and accurate answers by analyzing the discrepancies in chest X-ray images of the same patient across different time points. Compared to the previous method, ReAl additionally aid a residual input branch, where the residual of two images is fed into this branch. Additionally, a Residual Feature Alignment (RFA) module is introduced to ensure that ReAl effectively captures and learns the disparities between corresponding images. Experimental evaluations conducted on the MIMIC-Diff-VQA dataset demonstrate the superiority of ReAl over previous state-of-the-art methods, consistently achieving better performance. Ablation experiments further validate the effectiveness of the RFA module in enhancing the model’s attention to differences. The code implementation of the proposed approach will be made available.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2957_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2957_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/726-Paper2957.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/726-Paper2957.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="." target="_blank">Code</a><br><br>Despite the strong prediction power of deep learning models, their interpretability remains an important concern. Disentanglement models increase interpretability by decomposing the latent space into interpretable subspaces. In this paper, we propose the first disentanglement method for pathology images. We focus on the task of detecting tumor-infiltrating lymphocytes (TIL). We propose different ideas including cascading disentanglement, novel architecture and reconstruction branches. We achieve superior performance on complex pathology images, thus improving the interpretability and even generalization power of TIL detection deep learning models.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3843_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3843_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/688-Paper3843.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/688-Paper3843.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/Shauqi/SS-cVAE" rel="noopener" class="external-link" href="https://github.com/Shauqi/SS-cVAE" target="_blank">Code</a><br><img alt="ss-cvae.jpg" src="/conference-meeting-notes/miccai'24/images/ss-cvae.jpg"><br><br><br>Neural radiance field has recently emerged as a powerful representation to reconstruct deformable tissues from endoscopic videos. Previous methods mainly focus on depth-supervised approaches based on endoscopic datasets. As additional information, depth values were proven important in reconstructing deformable tissues by previous methods. However, collecting a large number of datasets with accurate depth values limits the applicability of these approaches for endoscopic scenes. To address this issue, we propose a novel self-supervised monocular 3D scene reconstruction method based on neural radiance fields without prior depth as supervision. We consider the monocular 3D reconstruction based on two approaches: ray-tracing-based neural radiance fields and structure-from-motion-based photogrammetry. We introduce structure from motion framework and leverage color values as a supervision to complete the self-supervised learning strategy. In addition, we predict the depth values from neural radiance fields and enforce the geometric constraint for depth values from adjacent views. Moreover, we propose a looped loss function to fully explore the temporal correlation between input images. The experimental results showed that the proposed method without prior depth outperformed the previous depth-supervised methods on two endoscopic datasets. Our code is available.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1656_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1656_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/276-Paper1656.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/276-Paper1656.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/MoriLabNU/EndoSelf" rel="noopener" class="external-link" href="https://github.com/MoriLabNU/EndoSelf" target="_blank">Code</a><br>
<img alt="EndoSelf.jpg" src="/conference-meeting-notes/miccai'24/images/endoself.jpg"><br><br>Three-dimensional reconstruction of the surgical area based on intraoperative laparoscopic videos can restore 2D information to 3D space, providing a solid technical foundation for many applications in computer-assisted surgery. SLAM methods often suffer from imperfect pose estimation and tissue motion, leading to the loss of original texture information. On the other hand, methods like Neural Radiance Fields and 3D Gaussian Split require offline processing and lack generalization capabilities. To overcome these limitations, we explore a texture optimization method that generates high resolution and continuous texture. It designs a mechanism for transforming 3D point clouds into 2D texture space and utilizes a generative network architecture to design 2D registration and image fusion modules. Experimental results and comparisons with state-of-the-art techniques demonstrate the effectiveness of this method in preserving the high-fidelity texture.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2885_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2885_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/518-Paper2885.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/518-Paper2885.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="" rel="noopener" class="external-link" href="." target="_blank">Code</a><br>
<img alt="Misaligned3D.jpg" src="/conference-meeting-notes/miccai'24/images/misaligned3d.jpg"><br><br>Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering synthetic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate structurally accurate simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN can imitate realistic endoscopic images from these simulations, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training networks and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures. The code will be made public on GitHub.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/510-Paper2259.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/510-Paper2259.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2259_paper.pdf" target="_blank">Code</a><br><img alt="Meshbrush.jpg" src="/conference-meeting-notes/miccai'24/images/meshbrush.jpg"><br><br>Dynamic reconstruction of deformable tissues in endoscopic video is a key technology for robot-assisted surgery. Recent reconstruction methods based on neural radiance fields (NeRFs) have achieved remarkable results in the reconstruction of surgical scenes. However, based on implicit representation, NeRFs struggle to capture the intricate details of objects in the scene and cannot achieve real-time rendering. In addition, restricted single view perception and occluded instruments also propose special challenges in surgical scene reconstruction. To address these issues, we develop SurgicalGaussian, a deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our approach models the spatio-temporal features of soft tissues at each time stamp via a forward-mapping deformation MLP and regularization to constrain local 3D Gaussians to comply with consistent movement. With the depth initialization strategy and tool mask-guided training, our method can remove surgical instruments and reconstruct high-fidelity surgical scenes. Through experiments on various surgical videos, our network outperforms existing method on many aspects, including rendering quality, rendering speed and GPU usage. The project page can be found at <a rel="noopener" class="external-link" href="https://surgicalgaussian.github.io" target="_blank">https://surgicalgaussian.github.io</a><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/3077_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/3077_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/745-Paper3077.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/745-Paper3077.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://surgicalgaussian.github.io." rel="noopener" class="external-link" href="https://surgicalgaussian.github.io." target="_blank">Code</a><br><img alt="SurgicalGaussian.jpg" src="/conference-meeting-notes/miccai'24/images/surgicalgaussian.jpg"><br><br>We introduce ColonSLAM, a system that combines classical multiple-map metric SLAM with deep features and topological priors to create topological maps of the whole colon. The SLAM pipeline by itself is able to create disconnected individual metric submaps representing locations from short video subsections of the colon, but is not able to merge covisible submaps due to deformations and the limited performance of the SIFT descriptor in the medical domain. ColonSLAM is guided by topological priors and combines a deep localization network trained to distinguish if two images come from the same place or not and the soft verification of a transformer-based matching network, being able to relate far-in-time submaps during an exploration, grouping them in nodes imaging the same colon place, building more complex maps than any other approach in the literature. We demonstrate our approach in the Endomapper dataset, showing its potential for producing maps of the whole colon in real human explorations. Code and models are available at: <a rel="noopener" class="external-link" href="https://github.com/endomapper/ColonSLAM" target="_blank">https://github.com/endomapper/ColonSLAM</a><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/1110_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/1110_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/778-Paper1110.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/778-Paper1110.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/endomapper/ColonSLAM" rel="noopener" class="external-link" href="https://github.com/endomapper/ColonSLAM" target="_blank">Code</a><br><img alt="Topological Slam.jpg" src="/conference-meeting-notes/miccai'24/images/topological-slam.jpg"><br><br>Deformation recovery from laparoscopic images benefits many downstream applications like robot planning, intraoperative navigation and surgical safety assessment. We define tissue deformation as time-variant surface structure and displacement. Besides, we also pay attention to the surface strain, which bridges the visual observation and the tissue biomechanical status, for which continuous pointwise surface mapping and tracking are necessary. Previous SLAM-based methods cannot cope with instrument-induced occlusion and severe scene deformation, while the neural field-based ones are offline and scene-specific, which hinders their application in continuous mapping. Moreover, neither approach meets the requirement of continuous pointwise tracking. To overcome these limitations, we assume a deformable environment and a movable window through which an observer depicts the environment’s 3D structure on a canonical canvas as maps in a process named impasto. The observer performs panoramic impasto for the currently and previously observed 3D structure in a two-step online approach: optimization and fusion. The optimization of the maps compensates for the error in the observation of the structure and the tracking by preserving spatiotemporal smoothness, while the fusion is for merging the estimated and the newly observed maps by ensuring visibility. Experiments were conducted using ex vivo and in vivo stereo laparoscopic datasets where tool-tissue interaction occurs and large camera motion exists. Results demonstrate that the proposed online method is robust to instrument-induced occlusion, capable of estimating surface strain, and can continuously reconstruct and track surface points regardless of camera motion. Code is available at: <a rel="noopener" class="external-link" href="https://github.com/bmpelab/trans_window_panoramic_impasto.git" target="_blank">https://github.com/bmpelab/trans_window_panoramic_impasto.git</a><br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/4075_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/4075_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/800-Paper4075.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/800-Paper4075.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/bmpelab/trans_window_panoramic_impasto.git" rel="noopener" class="external-link" href="https://github.com/bmpelab/trans_window_panoramic_impasto.git" target="_blank">Code</a><br><img alt="Transwindow.jpg" src="/conference-meeting-notes/miccai'24/images/transwindow.jpg"><br><br>Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of pre- cancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce “Gaussian Pancakes”, a method that lever- ages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100× faster rendering and more than 10× shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.<br><a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/paper/2298_paper.pdf" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/paper/2298_paper.pdf" target="_blank">Full Paper</a><br>
<a data-tooltip-position="top" aria-label="https://papers.miccai.org/miccai-2024/349-Paper2298.html" rel="noopener" class="external-link" href="https://papers.miccai.org/miccai-2024/349-Paper2298.html" target="_blank">Full Review</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/smbonilla/GaussianPancakes" rel="noopener" class="external-link" href="https://github.com/smbonilla/GaussianPancakes" target="_blank">Code</a>]]></description><link>conference-meeting-notes/miccai&apos;24/miccai&apos;24-recap.html</link><guid isPermaLink="false">Conference - Meeting Notes/Miccai&apos;24/MICCAI&apos;24 Recap.md</guid><pubDate>Wed, 16 Oct 2024 13:38:05 GMT</pubDate><enclosure url="conference-meeting-notes/miccai&apos;24/images/a-framework-for-assessing-joint-human-ai-systems-based-on-uncertainty-estimation.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;conference-meeting-notes/miccai&apos;24/images/a-framework-for-assessing-joint-human-ai-systems-based-on-uncertainty-estimation.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>